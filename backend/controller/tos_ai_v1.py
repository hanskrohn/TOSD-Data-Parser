# -*- coding: utf-8 -*-
"""ToS-AI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RC4DmEqRtPljZc2d-iDq0v6XMW8TdO0c

# Set up
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip3 install pymongo[srv]
# !pip install umap-learn

"""Import relevant packages"""

from absl import logging
from datetime import datetime

import tensorflow as tf

from sklearn import preprocessing 
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_absolute_error

import os
import pickle

import tensorflow_hub as hub
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import re
import seaborn as sns

import dns # allows use of mongo URI  
import pymongo
from pymongo import MongoClient
from bson.objectid import ObjectId # Allows us to build ObjectId objects

import umap
import plotly.express as px

import xgboost as xgb

"""Mount drive"""

from google.colab import drive
drive.mount('/content/drive')

"""Load Google Universal Sentence Encoder model"""

module_url = "https://tfhub.dev/google/universal-sentence-encoder/4"
sentence_encoder_model = hub.load(module_url)
print (f"module {module_url} loaded")

"""Hardcode ENV variable **Change when local

"""

ENV = {
    "mongoUsername": "hk72",
    "mongoPassword": "CapstoneProject2021"
}

"""Connect to Mongo DB"""

username = ENV["mongoUsername"]
password = ENV["mongoPassword"]
client = pymongo.MongoClient(f"mongodb+srv://{username}:{password}@cluster0.bildb.mongodb.net/myFirstDatabase?retryWrites=true&w=majority")
"mongodb+srv://hk72:CapstoneProject2021@cluster0.bildb.mongodb.net/myFirstDatabase?retryWrites=true&w=majority"

print(client.list_database_names())
db = client.myFirstDatabase

print(db)

"""Define helper functions"""

def embed(input):
  return sentence_encoder_model(input)

def print_embeddings(sentences, embeddings_list):
  for i, sentence_embedding in enumerate(np.array(embeddings_list).tolist()):
    print("Sentence: {}".format(sentences[i]))
    print("Embedding size: {}".format(len(sentence_embedding)))
    sentence_embedding_snippet = ", ".join(
        (str(x) for x in sentence_embedding[:3]))
    print("Embedding: [{}, ...]\n".format(sentence_embedding_snippet))

def plot_similarity(labels, features, rotation):
  corr = np.inner(features, features)
  sns.set(font_scale=1.2)
  g = sns.heatmap(
      corr,
      xticklabels=labels,
      yticklabels=labels,
      vmin=0,
      vmax=1,
      cmap="YlOrRd")
  g.set_xticklabels(labels, rotation=rotation)
  g.set_title("Semantic Textual Similarity")

def average(list_numbers):
  average = sum(list_numbers) / len(list_numbers)
  return average

def get_avg_similarity(features):
  corr = np.inner(features, features)
  
  avgs_list = []
  for row in corr:
    avgs_list.append(average(row))

  return average(avgs_list)

"""Define custom helper functions"""

"""
Compute the average similarity between a case
and all snippets that map to that case
RETURN: int, average similarity between case text and each snippet text
"""
def avg_sim_to_case(case_embedding, snippet_embeddings):
  
  similarity_arr = [] # store similarity values between snippet and case
  average_similarity = 0
  
  for snippet_embedding in snippet_embeddings:
    similarity = np.inner(snippet_embedding, case_embedding)
    similarity_arr.append(similarity)
  
  average_similarity = np.average(similarity_arr)

  return average_similarity

"""
Parse out text for all cases and the snippets that map to that case
RETURN: list of objects of form {case_test: "", service_snippets=["",""]}
"""
def get_all_cases_snippets_text():
  cases_snippets_text_arr = [] 
  
  collection = db.cleaneddataswithexclusion

  cases = list(collection.find())

  for doc in cases:
    case_dict = {}
    case_text = ""
    service_snippets = []

    case_text = doc["case"]

    snippets_arr = doc["snippets"]

    for snippet in snippets_arr:
      snippet_text = snippet["quoteText"]
      service_snippets.append(snippet_text)

    case_dict["case_text"] = case_text
    case_dict["service_snippets"] = service_snippets
    cases_snippets_text_arr.append(case_dict)
  
  return cases_snippets_text_arr

"""
Convert text to embeddings for all cases and snippets that map to that case
RETURN: list of objs in form {case_embedding: [], service_snippet_embeddings=[[],[]]} 
"""
def get_all_cases_snippets_embeddings(cases_snippets_text_arr):
  
  cases_snippets_embeddings_arr = [] 
  
  n = 0

  for case in cases_snippets_text_arr:
    case_embedded_dict = {}
    
    case_embedding = embed([case["case_text"]])
    service_snippet_embeddings = embed(case["service_snippets"])

    case_embedded_dict["case_embedding"] = case_embedding
    case_embedded_dict["case_text"] = case["case_text"]
    case_embedded_dict['case_num'] = n
    case_embedded_dict["service_snippet_embeddings"] = service_snippet_embeddings

    cases_snippets_embeddings_arr.append(case_embedded_dict)

    n += 1

  return cases_snippets_embeddings_arr

def get_unique_ordered(seq):
    seen = set()
    seen_add = seen.add
    return [x for x in seq if not (x in seen or seen_add(x))]

"""
placeholder
"""
def group_cases(data_arr, groupings):    
  case_num = data_arr[-1]['case_num'] + 1

  for group in groupings:
    label = group['label']
    case_nums = group['cases']

    cases_to_drop_temp = []
    all_snippets = None 
    temp_case = {}

    for case in clean_data_arr:
      if case['case_num'] in case_nums:
        cases_to_drop_temp.append(case)

    clean_data_arr[:] = [d for d in clean_data_arr if d.get('case_num') not in case_nums]

    for case in cases_to_drop_temp:
      if all_snippets == None:
        all_snippets = case['service_snippet_embeddings']
      else:
        all_snippets = tf.concat([all_snippets,  case['service_snippet_embeddings']], 0)

    temp_case['case_text'] = label
    temp_case['case_num'] = case_num
    temp_case['case_embedding'] = embed([label])
    temp_case['service_snippet_embeddings'] = all_snippets

    clean_data_arr.append(temp_case)

    case_num += 1

  return data_arr


"""

"""
def filter_num_snippets(data_arr, threshold_snippets):
  new_data_arr = [x for x in data_arr if x['service_snippet_embeddings'] != None 
                  and len(x['service_snippet_embeddings']) >= threshold_snippets]
  return new_data_arr

"""
Manually delete cases by case_num
RETURN: new arr of dicts with cases removed 
"""
def delete_cases(data_arr, case_nums):
  new_data_arr = [x for x in data_arr if x['case_num'] not in case_nums]
  return new_data_arr

"""
Print a summary including: Case text, number of service snippets that map to it,
  average similarity of case text to snippet (for each case)
RETURN: none
"""
def print_summary(cases_snippets_embeddings_arr): #probably translate this to a dataframe instead
  for embedded_case in cases_snippets_embeddings_arr:
    average_similarity = avg_sim_to_case(embedded_case["case_embedding"],embedded_case["service_snippet_embeddings"])
    
    print("Case text: " + embedded_case["case_text"])
    print("Number of service snippets: ", len(embedded_case["service_snippet_embeddings"]))
    print("Average similarity of snippet to case: ", average_similarity)
    print("________________________________________________________")

"""# Hard-coded Dry Run
Skip this section for ML

Get case text and snippets from DB
"""

case_text = ""
service_snippets = []
collection = db.cleaneddataswithexclusion

cookiesID = ObjectId("60cc1f22fc727e40b42439cf") # ID for cookies case

doc = collection.find( {'_id' : cookiesID })

case_text = doc[0]["case"] # treat returned cursor as Dict becuase it contains single obj

snippets_arr = doc[0]["snippets"]

for snippet in snippets_arr:
  snippet_text = snippet["quoteText"]
  service_snippets.append(snippet_text)

"""Create embeddings for case text and snippets"""

case_embedding = embed([case_text])
snippet_embeddings = embed(service_snippets)

print_embeddings([case_text], case_embedding)

print_embeddings(service_snippets, snippet_embeddings)

"""Visualize similarity within specific case"""

# plot_similarity(service_snippets, snippet_embeddings, 90) # Plot similarity with service text as labels

enumeration = list(range(1, len(service_snippets)))
plot_similarity(enumeration, snippet_embeddings, 90) # Plot similarity with enumerated snippets as labels

"""Check snippets similarity with case"""

avg_sim_to_case(case_embedding, snippet_embeddings)

# avg_sim_to_case([.5, .5, .5], [[.6, .6, .6], [.6, .6, .6], [.6, .6, .6]]) # sanity check

"""# Data Pre-processing

Get all case and snippet text - Starting point for ML
"""

cases_snippets_text_arr = get_all_cases_snippets_text() # List of objs in form {case_text: "", service_snippets=["",""]} 
print(len(cases_snippets_text_arr))
master_arr = get_all_cases_snippets_embeddings(cases_snippets_text_arr) # List of objs in form {case_text:"", case_embedding: [], service_snippet_embeddings=[[],[]]}

for case in master_arr:
    print(f'{case["case_num"]}-{case["case_text"]}')

"""NOTE: See input object below """

master_arr[0]
# """
# [
#  {
#     case_text: "", 
#     case_num: n,
#     case_embedding: [x1, x2, ..., x512], 
#     service_snippet_embeddings=[[x1, x2, ..., x512],[y1, y2, ..., y512], ... [zzz1, zzz2, ..., zzz512]],
#     average_snippet_embedding
#   } 
# ]
# """

"""Play around with numpy arrays"""

print(type(master_arr[0]["service_snippet_embeddings"].numpy()))

"""Convert tenors to 2D numpy Arrays"""

#List variables to temporarily hold features and labels
data_arr = []
labels_arr = []
case_embedding_arr = []
dict_for_master_df = []
#print(type(master_arr[0]["service_snippet_embeddings"].numpy()[0]))
#print(master_arr[0]["service_snippet_embeddings"].numpy()[0].shape)

for case_obj in master_arr:
  for service_snippet in case_obj["service_snippet_embeddings"].numpy():
    # labels_arr.append(case_obj["case_text"])
    data_arr.append(service_snippet)
    labels_arr.append(case_obj["case_text"])
  case_embedding_arr.append(case_obj["case_embedding"].numpy())
  
  # Convert tensors into numpy array 
  dict_for_master_df.append(case_obj["service_snippet_embeddings"].numpy())
  dict_for_master_df.append(case_obj["case_text"])

  
#Convert lists to numpy arrays
data = np.array(data_arr)  # Numpy array of snippet embedding vectors
label = np.array(labels_arr)  # String targets
case_embeddings = np.array(case_embedding_arr) # Numpy array of case embedding vectors

print("data: ", data)
print("labels: ",label)
# print("Num samples: ", len(data_arr))
# print("Num labels: ", len(labels_arr)) # Num should be same for samples and labels

# print("Samples shape ", data.shape) # Should be 2d
# print("Labels shape ", label.shape) # Should be 1d
# print("Case embeddings shape ", case_embeddings.shape) # Should be 1d

master_df = pd.DataFrame(dict_for_master_df)

# print("master: ", master_df)
#print("case_emb_arr: ", case_embedding_arr)

"""Create ordinal encoding for label set"""

label_encoder =  preprocessing.LabelEncoder()
label_ordinal = label_encoder.fit_transform(label) # encode label set as integers
print(label_ordinal)

"""# Feature Vector Visualization

EDA - exploratory data analysis
"""

cases_text = get_unique_ordered(label)
average_similarity_betweeen_snippets = []
average_similarity_to_case = []
num_snippets = []

for case_obj in master_arr:
  average_similarity_betweeen_snippets.append(get_avg_similarity(case_obj["service_snippet_embeddings"]))
  average_similarity_to_case.append(avg_sim_to_case(case_obj["case_embedding"], case_obj["service_snippet_embeddings"]))
  num_snippets.append(len(case_obj["service_snippet_embeddings"]))

EDA_df = pd.DataFrame(list(zip([str(x) for x in range(len(cases_text))], cases_text, num_snippets, average_similarity_betweeen_snippets, average_similarity_to_case)), columns=["Case_Num", "Case_Text", "Number_of_Snippets", "Average_Similarity_Between_Snippets", "Average_Similarity_to_Case"])
EDA_df.to_csv("/content/drive/MyDrive/Capstone/EDA.csv")

def stringy(x):
    return str(x)
labs = np.vectorize(stringy)(label_ordinal)
print(labs)

umap_2d = umap.UMAP(random_state=0)
umap_2d.fit(data)

projections = umap_2d.transform(data)

fig_2d = px.scatter(
    projections, x=0, y=1,
    color=labs,
    # color=label,
)

# Hover the top-right for analysis tools
# You can click on any individual label in the legend to toggle whether to display those data points
fig_2d

fig_2d_text_mapping = px.scatter(
    projections, x=0, y=1,
    # Use 'colour=labs' for a less descriptive but more navigable graph
    # color=labs,
    color=label,
)

fig_2d_text_mapping

umap_3d = umap.UMAP(random_state=0, n_components=3)
umap_3d.fit(data)

projections = umap_3d.transform(data)

fig_3d = px.scatter_3d(
    projections, x=0, y=1, z=2,
    # Use 'colour=label' for a more descriptive but less navigable graph
    color=labs,
)

# Hover the top-right for analysis tools
# You can click on any individual label in the legend to toggle whether to display those data points
fig_3d

"""
# Plot case text embeddings
"""

umap_2d = umap.UMAP(random_state=0)
case_embeddings_map = [case_obj["case_embedding"][0].numpy() for case_obj in master_arr]
case_embeddings_map = np.array(case_embeddings_map)  # Numpy array of embedding vectors

umap_2d.fit(case_embeddings_map)
projections = umap_2d.transform(case_embeddings_map)

# labs_unique = np.vectorize(stringy)(label_ordinal)

fig_2d = px.scatter(
    projections, x=0, y=1,
    color= [str(x) for x in range(len(case_embeddings_map))],
)

fig_2d

"""Plot all snippets as single vector to understand which cases have similar between cases and which may cause issues

"""

umap_2d = umap.UMAP(random_state=0)
snippet_means = np.array([np.mean(case_obj["service_snippet_embeddings"].numpy(), axis=0) for case_obj in master_arr])

umap_2d.fit(snippet_means)
projections = umap_2d.transform(snippet_means)

# labs_unique = np.vectorize(stringy)(label_ordinal)

fig_2d = px.scatter(
    projections, x=0, y=1,
    color= [str(x) for x in range(len(snippet_means))],
)

fig_2d

"""Print Case Text Mapping to Encoded Label"""

for index, value in zip(range(len(cases_text)), cases_text):
    print (index, '-', value)

"""# Remove and merge cases

Manually delete cases
"""

# manually_deleted = get_manually_deleted() # ex. format [0, 1, 2, 3, 4]

manually_deleted = [237, 21, 100]

clean_data_arr = delete_cases(master_arr, manually_deleted)

for case in clean_data_arr:
  print(f'{case["case_num"]}-{len(case["service_snippet_embeddings"])}')

# groupings = get_to_group() # ex. format [{label: "xyz", cases: [0, 1]}, {}]

groupings = [{'label': "The terms may be changed at any time, but you will receive notification of the changes", 'cases':[18, 101, 150, 151, 200]}, 
             {'label': "Terms may be changed any time at their discretion, without notice to you ", 'cases':[40, 22, 2]}, 
             {'label': "The service provider makes no warranty regarding uninterrupted, timely, secure or error-free service", 'cases':[81, 82, 95, 48, 91, 119]},
             {'label': "Your content can be distributed through any media known now or in the future", 'cases':[115, 8, 213, 136]},
             {'label': "You agree not to submit libelous, harassing or threatening content", 'cases':[152, 153, 165, 73]},
             {'label': "There is a date of the last update of the agreements", 'cases':[16, 102]},
             {'label': "Your personal data may be used for marketing purposes", 'cases':[219, 224, 226, 190, 228]}]

clean_data_arr = group_cases(clean_data_arr, groupings)


n = 0
for case in clean_data_arr:
  n= n+1
  print(f'{case["case_num"]}-{len(case["service_snippet_embeddings"])}')

n

filtered_cases = EDA_df[EDA_df.Average_Similarity_Between_Snippets < 0.25 ]['Case_Num']

clean_data_arr = filter_num_snippets(clean_data_arr, 30)

clean_data_arr = delete_cases(clean_data_arr, filtered_cases)

i = 0
for case in clean_data_arr:
  i=i+1
  print(f'{case["case_num"]}-{len(case["service_snippet_embeddings"])}')

print (i)

new_clean_data_arr=pd.DataFrame(data=clean_data_arr)
new_clean_data_arr.to_csv("/content/drive/MyDrive/Capstone/clean_data.csv")

"""Convert data arr back to numpy data and labels for model training"""

#List variables to temporarily hold features and labels
data_arr_for_ML = []
labels_arr_for_ML = []

for case_obj in clean_data_arr:
  for service_snippet in case_obj["service_snippet_embeddings"].numpy():
    data_arr_for_ML.append(service_snippet)
    labels_arr_for_ML.append(case_obj["case_text"])
  
#Convert lists to numpy arrays
data_for_ML = np.array(data_arr_for_ML)  # Numpy array of snippet embedding vectors
label_for_ML = np.array(labels_arr_for_ML)  # String targets

print("data: ", data_for_ML)
print("labels: ", label_for_ML)
print (len(set(label_for_ML)))
print (len(set(labels_arr_for_ML)))
#print (len(set(label_for_ML)))
print("Num samples: ", len(data_arr_for_ML))
print("Num labels: ", len(labels_arr_for_ML)) # Num should be same for samples and labels

print("Samples shape ", data_for_ML.shape) # Should be 2d
print("Labels shape ", label_for_ML.shape) # Should be 1d
# print("Case embeddings shape ", case_embeddings.shape) # Should be 1d

new_labels_for_ML=pd.DataFrame(data=label_for_ML)
new_labels_for_ML.to_csv(f"/content/drive/MyDrive/Capstone/Models and Reports/{training_datetime, param_info}/LabelForML.csv")

"""TO DO - Make modular method of training model based on latest subset of data

# Train basic classifer

Stratify data to test and seed sets
"""

label_encoder_for_ML =  preprocessing.LabelEncoder()
label_ordinal_for_ML = label_encoder_for_ML.fit_transform(label_for_ML) # encode label set as integers
print(label_ordinal_for_ML)
print(len(set(label_ordinal_for_ML)))
print(set(label_ordinal_for_ML))
#pd.DataFrame(np.array(set(label_ordinal_for_ML))).to_csv(f"/content/drive/MyDrive/Capstone/Models and Reports/LabelForMLNew.csv")

unique, counts = np.unique(label_for_ML, return_counts=True)
dict(zip(unique, counts))

seed = 7 # ensures that split is reproducible
test_size_in = 0.2 # split of training vs testing data

X_train, X_test, y_train, y_test = train_test_split(data_for_ML, label_ordinal_for_ML, test_size=test_size_in, 
                                                    random_state=seed, stratify=label_ordinal_for_ML)

print(f'train set length: {len(X_train)}')
print(f'test set length: {len(X_test)}')

# fit model on training data
model = xgb.XGBClassifier()
num_classes = len(set(label_for_ML))

print(num_classes)

"""Hyperparameter Tuning: Mean Absolute Error to evaluate quality of predictions compared against baseline model

"""

# "Learn" the mean from the training data
mean_train = np.mean(y_train)

# Get predictions on the test set
baseline_predictions = np.ones(y_test.shape) * mean_train

# Compute MAE
mae_baseline = mean_absolute_error(y_test, baseline_predictions)

print("Baseline MAE is {:.2f}".format(mae_baseline))

"""Train the model!"""

dtrain = xgb.DMatrix(X_train, y_train)
param = {'max_depth':7, 'n_estimators':1000, 'eta':1, 'verbosity':3, 'objective':'multi:softmax', 'num_class': num_classes }
num_round = 2
bst = xgb.train(param, dtrain, num_round)

"""Create folder for current model"""

training_datetime = datetime.now().strftime("%Y%m%d%H%M%S")
param_info = param;

os.mkdir(f"/content/drive/MyDrive/Capstone/Models and Reports/{training_datetime, param_info}")

# Save summary of data
summary_df = pd.DataFrame([[case["case_text"], len(case["service_snippet_embeddings"])] for case in clean_data_arr])
summary_df.to_csv(f"/content/drive/MyDrive/Capstone/Models and Reports/{training_datetime, param_info}/Data_Summary.csv")

model = bst

def call(model, inf):
    return model.predict(xgb.DMatrix(inf))

"""Save serialized model to file"""

with open(f"/content/drive/MyDrive/Capstone/Models and Reports/{training_datetime, param_info}/model.pickle.dat", "wb") as ofile:
  pickle.dump(model, ofile)

"""Test model"""

# make predictions for test data
y_pred = call(model, X_test)
predictions = [round(value) for value in y_pred]
# evaluate predictions
accuracy = accuracy_score(y_test, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))

#save accuracy value to text file in folder
with open(f"/content/drive/MyDrive/Capstone/Models and Reports/{training_datetime, param_info}/notes.txt", "w") as writefile:
    writefile.write("Accuracy: %.2f%%" % (accuracy * 100.0))

"""Get classification report and save to drive"""

# Transform ordinal labels to case text 
test_labels = label_encoder_for_ML.inverse_transform(y_test)
saved_label_encoder_for_ML=dict(zip(label_encoder_for_ML.classes_, label_encoder_for_ML.transform(label_encoder_for_ML.classes_)))
pd.DataFrame(saved_label_encoder_for_ML, index=[0]).to_csv(f"/content/drive/MyDrive/Capstone/Models and Reports/savedLabelEncoder.csv")

# Create list for target labels
#target_names = set(labels_arr_for_ML).intersection(set(test_labels))
target_names = list(set(labels_arr_for_ML))

# Create pandas dataframe for classification report
classification_report_arr = classification_report(y_test, y_pred, target_names=label_encoder_for_ML.classes_, output_dict=True)
classification_report_df = pd.DataFrame(classification_report_arr).transpose()

# Save classification report as CSV
classification_report_df.to_csv(f'/content/drive/MyDrive/Capstone/Models and Reports/{training_datetime, param_info}/Classification_Report.csv')

"""Build confusion matrix and save to CSV"""

y_true = label_encoder_for_ML.inverse_transform(y_test)

y_pred_int = [int(x) for x in y_pred]
y_pred_text = label_encoder_for_ML.inverse_transform(y_pred_int)

confusion_numpy = confusion_matrix(y_true, y_pred_text, labels=label_encoder_for_ML.classes_)

confusion_df = pd.DataFrame(data=confusion_numpy)

confusion_df.to_csv(f'/content/drive/MyDrive/Capstone/Models and Reports/{training_datetime, param_info}/Confusion_Matrix.csv')

"""Test Random Texts and see which case is maps to"""

test_text = "Cookies"
test_embedding = embed([test_text])

# Convert to numpy array
test_numpy = test_embedding.numpy()

#Create DMatrix
dtest = xgb.DMatrix(test_numpy)

prediction_num = model.predict(dtest)

print(prediction_num)

label_encoder_for_ML.inverse_transform([int(prediction_num)])

"""Load saved model"""

# load serialized model from file
model_datetime = "('20210818155130', {'max_depth': 7, 'n_estimators': 1000, 'eta': 1, 'verbosity': 3, 'objective': 'multi:softmax', 'num_class': 38})"
loaded_model = pickle.load(open(f"/content/drive/MyDrive/Capstone/Models and Reports/{model_datetime}/model.pickle.dat", "rb"))
print(loaded_model)

"""Test model with new string"""

test_text = "This service will use your cookies how it wishes."
test_embedding = embed([test_text])

# Convert to numpy array
test_numpy = test_embedding.numpy()

#Create DMatrix
dtest = xgb.DMatrix(test_numpy)

prediction_num = loaded_model.predict(dtest)

print(prediction_num)

label_encoder.inverse_transform([int(prediction_num)])

"""Test Loaded model"""

# make predictions for test data
y_pred = call(loaded_model, X_test)
predictions = [round(value) for value in y_pred]
# evaluate predictions
accuracy = accuracy_score(y_test, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))